{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abead0e1-97ea-4b52-ba63-247f67e04f36",
   "metadata": {},
   "source": [
    "## EST.QA.1\n",
    "\n",
    "\n",
    "Let ${\\mathbf z} $ have a mixture of gaussians distribution \n",
    "where the two components have the distributions  $ {\\mathbf z}_1 \\sim N(\\mu_1,\\sigma^2_1) $ and \n",
    "$ {\\mathbf z}_2 \\sim N(\\mu_2,\\sigma^2_2) $ and $ w_2=1-w_1$.\n",
    "\n",
    "Let us consider the dataset of $ N$ observations of $ {\\mathbf z} $:\n",
    "$$ D_N = [ 1.3, 0.85, 1, 1,  -0.83, 1.3, 1.3, 0.74, -1.2, -0.9, -0.93, 0.0, -1.14 ] $$\n",
    "\n",
    "Suppose that $w_1 \\in W_1 =\\{ 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0 \\} $\n",
    "\n",
    "Code a Python script to find the value of $w_1 \\in W_1 $ which maximises\n",
    "the likelihood of the dataset if $ \\mu_1=-1, \\sigma_1=0.1, \\mu_2=1, \\sigma_2=0.1 $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1928ae9f-625f-49b8-98d4-125e2c742da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Python code\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "mu1 = -1\n",
    "sd1 = 0.1\n",
    "mu2 = 1\n",
    "sd2 = 0.1\n",
    "\n",
    "def mixtdens(x, mu1, sd1, w1, mu2, sd2, w2):\n",
    "    return w1 * norm.pdf(x, mu1, sd1) + w2 * norm.pdf(x, mu2, sd2)\n",
    "\n",
    "\n",
    "DN = np.array([1.3, 0.85, 1, 1,  -0.83, 1.3, 1.3, 0.74, -1.2, -0.9, -0.93, 0.0, -1.14])\n",
    "N = len(DN)\n",
    "W1 = np.arange(0, 1.1, 0.1)\n",
    "\n",
    "L = []\n",
    "for w1 in W1:\n",
    "\n",
    "    lik = 1\n",
    "    for i in range(N):\n",
    "        lik *= mixtdens(DN[i], mu1, sd1, w1, mu2, sd2, 1 - w1)\n",
    "    L.append(lik)\n",
    "\n",
    "w1=W1[np.argmax(L)]\n",
    "print('bestw=',w1)\n",
    "\n",
    "\n",
    "# Plot the histogram of DN (equivalent to hist(DN) in R)\n",
    "plt.hist(DN)\n",
    "\n",
    "\n",
    "\n",
    "# Create a sequence from -3 to 3 with increments of 0.1 (equivalent to seq(-3, 3, by = 0.1) in R)\n",
    "z = np.arange(-2, 3.1, 0.01)\n",
    "\n",
    "# Plot the line on the current plot.\n",
    "# The expression w1*dnorm(z, -0.5, 1) + (1 - dnorm(z, 1, 1)) in R is translated using norm.pdf from scipy.stats\n",
    "plt.plot(z, w1 * norm.pdf(z, loc=mu1, scale=sd1) + (1 - w1)*norm.pdf(z, loc=mu2, scale=sd2))\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5713ebd-4110-4ef9-b8b1-11c4dd1f11dc",
   "metadata": {},
   "source": [
    "## EST.QA.2\n",
    "\n",
    "Let us consider a univariate random variable ${\\mathbf z}$ with Normal distribution $N(\\mu,\\sigma^2)$, where $\\mu=1 \\text{ and } \\sigma=1 $. \n",
    "\n",
    "Let $D_N={z_1,...,z_N}$ a i.i.d. dataset of size $N=100$ sampled from this distribution.\n",
    "\n",
    "By implementing a Monte Carlo simulation with 10000 trials in Python, the student should compute the bias, variance and MSE of the following four estimators of the ratio $\\frac{\\mu}{\\sigma}$: \n",
    "\n",
    "1. $\\hat{\\theta}_1= \\frac{\\hat{\\mu}}{\\hat{\\sigma}}$\n",
    "2. $ \\hat{\\theta}_2= \\sum_{i=1}^N z_i $\n",
    "3. $ \\hat{\\theta}_3= \\frac{\\sum_{i=1}^N (z_i-\\hat{\\mu})^3}{N \\hat{\\sigma}^3} $\n",
    "4. $ \\hat{\\theta}_4= \\frac{\\sum_{i=1}^N (z_i-\\hat{\\mu})^4}{N \\hat{\\sigma}^4} $\n",
    "where $ \\hat{\\mu}$ is the sample average and $\\hat{\\sigma}^2$  is the sample variance.\n",
    "\n",
    " \n",
    "\n",
    "The student should answer the following questions:\n",
    "\n",
    "1. Absolute value of bias of estimator 1 \n",
    "\n",
    "2. Absolute value of bias of estimator 2 \n",
    "\n",
    "3. Absolute value of bias of estimator 3 \n",
    "\n",
    "4. Absolute value of bias of estimator 4 \n",
    "\n",
    "5. Which estimator has the highest variance? \n",
    "  \n",
    "6. Which estimator has the highest MSE? \n",
    "7. Which estimator has the lowest MSE? \n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cadfdf7-40b7-46fa-a046-119b5b5991b4",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11328657-24b4-44a7-a827-3ee9d1d52be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "N = 100\n",
    "S = 10000\n",
    "mu = 1\n",
    "sdev = 1\n",
    "th = mu / sdev \n",
    "\n",
    "TH = None\n",
    "# Initialize TH as an empty list to accumulate columns of results\n",
    "TH_list = []\n",
    "\n",
    "for s in range(1, S + 1):\n",
    "    \n",
    "    DN = np.random.normal(mu, sdev, N)\n",
    "    that = []\n",
    "    muhat = np.mean(DN)\n",
    "    sdhat = np.std(DN, ddof=1)\n",
    "    \n",
    "    for num in range(1, 5):\n",
    "        if num == 1:\n",
    "            that.append(np.mean(DN) / np.std(DN, ddof=1))\n",
    "        if num == 2:\n",
    "            that.append(np.sum(DN))\n",
    "        if num == 3:\n",
    "            that.append(np.mean((DN - muhat) ** 3) / (sdhat ** 3))\n",
    "        if num == 4:\n",
    "            that.append(np.mean((DN - muhat) ** 4) / (sdhat ** 4))\n",
    "        TH_list.append(that)\n",
    "\n",
    "TH = np.array(TH_list).T\n",
    "\n",
    "B2 = None\n",
    "V = None\n",
    "MSE = None\n",
    "\n",
    "for num in range(1, 5):\n",
    "    # Calculate bias as the absolute difference between mean(TH[num,]) and th\n",
    "    curr_mean = np.mean(TH[num - 1, :])\n",
    "    # Calculate variance using sample variance (ddof=1) to match R's var()\n",
    "    curr_var = np.var(TH[num - 1, :], ddof=1)\n",
    "    curr_mse = np.mean((TH[num - 1, :] - th) ** 2)\n",
    "    sys.stdout.write(\"est= {} : |Bias|= {} Var= {} MSE= {}\\n\".format(num, abs(curr_mean - th), curr_var, curr_mse))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e50b8d-0121-44e5-8525-97176e931132",
   "metadata": {},
   "source": [
    "## EST.QA.3\n",
    "\n",
    "\n",
    "Let us consider a univariate random variable $\\mathbf z$ with Normal distribution $N(\\mu,\\sigma^2)$ where $\\mu=1 \\text{ and } \\sigma=1 $.  It is known that the kurtosis of a Normal distribution is equal to 3.\n",
    "\n",
    "Let $D_N={z_1,...,z_N} $ a i.i.d. dataset of size N=100 sampled from this distribution.\n",
    "\n",
    "By implementing a Monte Carlo simulation with 10000 trials in R, the student should compute the bias, variance and MSE of the following four estimators of the kurtosis: \n",
    "\n",
    "1. $ \\hat{\\theta}_1= \\frac{\\hat{\\mu}}{\\hat{\\sigma}}$\n",
    "2. $\\hat{\\theta}_2= \\sum_{i=1}^N z_i $\n",
    "3. $\\hat{\\theta}_3= \\frac{\\sum_{i=1}^N (z_i-\\hat{\\mu})^3}{N \\hat{\\sigma}^3} $\n",
    "4. $\\hat{\\theta}_4= \\frac{\\sum_{i=1}^N (z_i-\\hat{\\mu})^4}{N \\hat{\\sigma}^4} $\n",
    "where $ \\hat{\\mu} $ is the sample average and $\\hat{\\sigma}^2$ is the sample variance.\n",
    "\n",
    " \n",
    "\n",
    "The student should answer the following questions:\n",
    "\n",
    "1. Absolute value of bias of estimator 1 \n",
    "2. Absolute value of bias of estimator 2 \n",
    "3. Absolute value of bias of estimator 3 \n",
    "4. Absolute value of bias of estimator 4 \n",
    "5. Which estimator has the highest variance? \n",
    "6. Which estimator has the highest MSE? \n",
    "7. Which estimator has the lowest MSE? \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c25288-11c8-41cf-b2ea-a470e544cd24",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2102315-50d8-442d-a318-043770c3571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 100\n",
    "S = 10000\n",
    "\n",
    "mu = 1\n",
    "sdev = 1\n",
    "\n",
    "NTH = [12, 11, 2, 1]\n",
    "th = 3  ## kurtosis \n",
    "\n",
    "\n",
    "TH = []  # will store each simulation result as a list (each element is a column vector)\n",
    "for s in range(1, S+1):\n",
    "    DN = np.random.normal(mu, sdev, N)\n",
    "    \n",
    "    muhat = np.mean(DN)\n",
    "    sdhat = np.std(DN, ddof=1)\n",
    "    thetahat1 = np.mean(DN) / np.std(DN, ddof=1)\n",
    "    thetahat2 = np.sum(DN)\n",
    "    thetahat3 = np.mean((DN - muhat)**3) / sdhat**3\n",
    "    thetahat4 = np.mean((DN - muhat)**4) / sdhat**4\n",
    "    that = [thetahat1, thetahat2, thetahat3, thetahat4]\n",
    "    TH.append(that)\n",
    "\n",
    "# Convert TH to a numpy array of shape (S, 4) then transpose to shape (4, S) to mimic R's cbind behavior\n",
    "TH = np.array(TH).T\n",
    "\n",
    "B2 = []\n",
    "V = []\n",
    "MSE = []\n",
    "for num in range(1, len(NTH)+1):\n",
    "    est_mean = np.mean(TH[num-1, :])\n",
    "    est_var = np.var(TH[num-1, :], ddof=1)\n",
    "    bias = abs(est_mean - th)\n",
    "    mse = np.mean((TH[num-1, :] - th)**2)\n",
    "    B2.append(bias)\n",
    "    V.append(est_var)\n",
    "    MSE.append(mse)\n",
    "    print(\"theta=\", th, \" est=\", num, \": |B|=\", bias, \"V=\", est_var, \"MSE=\", mse)\n",
    "print(\"\\n --\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b347a1-fd5e-4909-b3b5-931bc7109c19",
   "metadata": {},
   "source": [
    "## EST.QA.4\n",
    "Let $ {\\mathbf z} \\sim U(-2,u) $ an uniformly distributed random variable between -2 and $u$.\n",
    "\n",
    "Let us consider the dataset of $N=10$ observations of $ {\\mathbf z} $:\n",
    "$ D_N = [ 1.3, -0.3,  1.0,  -1.3,  0.4, -1.5, -0.9, -0.3,  0.1,  2.4 ] $\n",
    "\n",
    "Suppose that $u \\in U =\\{ -1.5, -1.0, -0.5,  0.0 , 0.5,  1.0,  1.5 , 2.0,  2.5 , 3.0\\} $\n",
    "\n",
    "Code an Python script to find the value of $u \\in U $, which maximises\n",
    "the likelihood of the dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f2641-ed64-421d-bf28-ada386033f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python code\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "np.random.seed(0)\n",
    "low = -2\n",
    "def dens(x, low, up):\n",
    "    return uniform.pdf(x, loc=low, scale=up - low)\n",
    "\n",
    "N = 10\n",
    "DN = [1.3, -0.3, 1.0, -1.3, 0.4, -1.5, -0.9, -0.3, 0.1, 2.4]\n",
    "UP = np.arange(-1.5, 3.0 + 0.5, 0.5)\n",
    "L = []\n",
    "\n",
    "for up in UP:\n",
    "    lik = 1\n",
    "    for i in range(N):\n",
    "        lik *= dens(DN[i], low, up)\n",
    "    L.append(lik)\n",
    "\n",
    "print(UP[np.argmax(L)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8c1bd2-82c9-4a2a-b98e-cc399b431c65",
   "metadata": {},
   "source": [
    "## EST.QA.5\n",
    "Consider a binary random variable $\\mathbf y \\in \\{0,1\\}$ and a continuous random variable $\\mathbf x$.\n",
    "\n",
    "Suppose that \n",
    "$P(\\mathbf y =1 | x )= \\frac{1}{1+\\exp^{-wx}}$ where $w$ is integer that takes value in $[-10,10]$.\n",
    "\n",
    "Consider a dataset $D$ generated by the Python code here below.\n",
    "\n",
    "1. Find the value of $w$ that maximises the likelihood of the observed dataset.\n",
    "2. Find the value of $w$ that maximises the likelihood of the observed dataset by supposing that $w$ is continuous and using the python function [minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)\n",
    "\n",
    "\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed4494-3564-4c0d-be06-791272b122cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "N=50\n",
    "mu1=1\n",
    "sdev1=1\n",
    "D1=np.random.normal(mu1, sdev1, N)\n",
    "\n",
    "mu2=-1\n",
    "sdev2=1\n",
    "D2=np.random.normal(mu2, sdev2, N)\n",
    "\n",
    "D = np.column_stack((np.concatenate((D1, D2)),\n",
    "                     np.concatenate((np.zeros(N), np.zeros(N) + 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f34dcb-0231-4768-8258-db3c0ed56a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3536553-9bbd-4ec3-8099-97dee4296963",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db0e4fab-7947-4d7d-a96f-4f4bae1f61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(mlw)\n\u001b[1;32m     35\u001b[0m x\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mscatter(D[:,\u001b[38;5;241m0\u001b[39m],D[:,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     37\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x,condprob(x,mlw))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def condprob(x,w):\n",
    "    ## P(y=1 |x)\n",
    "    return(1/(1+np.exp(-w*x)))\n",
    "    \n",
    "\n",
    "## Dataset generation\n",
    "N=50\n",
    "mu1=1\n",
    "sdev1=1\n",
    "D1=np.random.normal(mu1, sdev1, N)\n",
    "mu2=-1\n",
    "sdev2=1\n",
    "D2=np.random.normal(mu2, sdev2, N)\n",
    "D = np.column_stack((np.concatenate((D1, D2)),\n",
    "                     np.concatenate((np.zeros(N), np.zeros(N) + 1))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N=D.shape[0]\n",
    "W = np.arange(-10, 10, 1)\n",
    "L = []\n",
    "for w in W:\n",
    "    lik = 1\n",
    "    for i in range(N):\n",
    "        if (D[i,1]==1):\n",
    "            lik *= condprob(D[i,0], w)\n",
    "        else:\n",
    "            lik *= (1-condprob(D[i,0], w))\n",
    "    L.append(lik)\n",
    "\n",
    "mlw=W[np.argmax(L)]\n",
    "print(mlw)\n",
    "\n",
    "x=np.arange(-5, 5, 0.1)\n",
    "plt.scatter(D[:,0],D[:,1])\n",
    "plt.plot(x,condprob(x,mlw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64d4f3f5-ce85-4a52-a991-5d9695e8fae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max-likelihood value of w: [-2.11770013]\n"
     ]
    }
   ],
   "source": [
    "## Solution with use of the function minimize\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def condprob(x,w):\n",
    "    ## P(y=1 |x)\n",
    "    return(1/(1+np.exp(-w*x)))\n",
    "    \n",
    "# Define the function to minimize\n",
    "\n",
    "## log-likelihood function\n",
    "def LL(w,D):\n",
    "    N=D.shape[0]\n",
    "    lik = 1\n",
    "    for i in range(N):\n",
    "        if (D[i,1]==1):\n",
    "            lik += np.log(condprob(D[i,0], w))\n",
    "        else:\n",
    "            lik += np.log((1-condprob(D[i,0], w)))\n",
    "    \n",
    "    return -lik\n",
    "\n",
    "# Initial guess\n",
    "initial_w = 0\n",
    "wbounds = [(-5, 5)]\n",
    "\n",
    "\n",
    "## Dataset generation\n",
    "N=50\n",
    "mu1=1\n",
    "sdev1=1\n",
    "D1=np.random.normal(mu1, sdev1, N)\n",
    "mu2=-1\n",
    "sdev2=1\n",
    "D2=np.random.normal(mu2, sdev2, N)\n",
    "D = np.column_stack((np.concatenate((D1, D2)),\n",
    "                     np.concatenate((np.zeros(N), np.zeros(N) + 1))))\n",
    "\n",
    "\n",
    "# Perform the minimization\n",
    "result = minimize(LL, initial_w,bounds=wbounds,args=D)\n",
    "\n",
    "# Print the results\n",
    "print(\"Max-likelihood value of w:\", result.x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd658500-d9b4-4d08-b3dd-29081b877f08",
   "metadata": {},
   "source": [
    "## EST.QA.6\n",
    "\n",
    "Let $\\mathbf x \\sim N(0,1)$ and $\\mathbf y= 3 \\mathbf x+ \\mathbf w$  where $\\mathbf w \\sim N(0,0.1)$.\n",
    "\n",
    "In other terms, the conditional density of $\\mathbf y$ given $\\mathbf x=x$ is $N(3 x,0.1)$.\n",
    "\n",
    "Generate a dataset of $N=100$ observations $x_i,y_i$.\n",
    "\n",
    "Find the maximum likelihood value of\n",
    "$b$ in the model $\\mathbf y= b \\mathbf x$ where $b \\in [-5,5]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55752f98-8745-4f3a-a979-d8b108d7cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "## Data generation\n",
    "N=100\n",
    "X=np.random.normal(size=N)\n",
    "varw=0.1\n",
    "Y=3*X.reshape(N,1)+np.random.normal(0,np.sqrt(varw),N)\n",
    "\n",
    "D = np.column_stack((X,Y))\n",
    "\n",
    "def LL(w,D):\n",
    "    N=D.shape[0]\n",
    "    lik = 0\n",
    "    for i in range(N):\n",
    "            lik += np.log(norm.pdf(D[i,1], loc=w*D[i,0], scale=np.sqrt(varw)))\n",
    "    \n",
    "    return -lik\n",
    "\n",
    "# Initial guess\n",
    "initial_b = 0\n",
    "bbounds = [(-5, 5)]\n",
    "\n",
    "# Perform the minimization\n",
    "result = minimize(LL, initial_b,bounds=bbounds,args=D)\n",
    "\n",
    "# Print the results\n",
    "print(\"Max-likelihood value of b:\", result.x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a831687-4402-4aaf-afb4-b8d5f1bf31eb",
   "metadata": {},
   "source": [
    "## EST.QA.7\n",
    "We want to estimate expected mean of a Gaussian r.v. \n",
    "${\\mathbf z} \\sim N(\\theta,\\sigma^2)$ where $\\theta=5$ and $\\sigma=2$\n",
    "using a dataset of size $N=50$.\n",
    "\n",
    "Consider a regularized estimator \n",
    "\n",
    "$$\\hat{\\theta}_r= \\lambda \\theta_0 +(1-\\lambda) \\frac{\\sum_{i=1}^{N'} z_i}{N'}$$ \n",
    "where $\\theta_0=3$, $N' <N$.\n",
    "\n",
    "Study the impact of the value of $\\lambda$ on bias, variance and MSE of $\\hat{\\theta}_r$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe53992-fcda-4f87-9c40-e7dc61e03cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "def generate_samples(true_mean, std_dev, N, R):\n",
    "    \"\"\"Generate multiple sets of samples from a normal distribution.\"\"\"\n",
    "    return np.random.normal(true_mean, std_dev, size=(R, N))\n",
    "\n",
    "def regularized_mean(DN, theta0, lam,Nprime):\n",
    "    R,N=DN.shape\n",
    "    if Nprime >=N:\n",
    "        raise ValueError(\"Error: N'>=N.\")       \n",
    "    estim = np.mean(DN[:,:Nprime],1)\n",
    "    return (lam*theta0+(1-lam)*estim) \n",
    "\n",
    "def calculate_metrics(estimates, theta):\n",
    "    \"\"\"Calculate bias, variance, and MSE of the estimator.\"\"\"\n",
    "    bias = np.mean(estimates) - theta\n",
    "    variance = np.var(estimates)\n",
    "    mse = np.mean((estimates-theta)**2)\n",
    "    return bias, variance, mse\n",
    "\n",
    "# Simulation parameters\n",
    "theta = 5.0\n",
    "std_dev = 2.0\n",
    "N = 50\n",
    "R = 1000\n",
    "lambda_values = np.linspace(0, 1, 20)\n",
    "theta0=3\n",
    "Nprime=2\n",
    "\n",
    "# Store results\n",
    "results = {\n",
    "    'bias': [],\n",
    "    'variance': [],\n",
    "    'mse': []\n",
    "}\n",
    "\n",
    "# Generate data once\n",
    "DN = generate_samples(theta, std_dev, N, R)\n",
    "\n",
    "# Calculate metrics for different regularization strengths\n",
    "for lambda_reg in lambda_values:\n",
    "    \n",
    "    \n",
    "    estimates = regularized_mean(DN, theta0, lambda_reg,Nprime)\n",
    "    bias, variance, mse = calculate_metrics(estimates, theta)\n",
    "    \n",
    "    results['bias'].append(bias)\n",
    "    results['variance'].append(variance)\n",
    "    results['mse'].append(mse)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot bias, variance, and MSE\n",
    "plt.plot(lambda_values, np.square(results['bias']), \n",
    "         label='Squared Bias', linestyle='--')\n",
    "plt.plot(lambda_values, results['variance'], \n",
    "         label='Variance', linestyle=':')\n",
    "plt.plot(lambda_values, results['mse'], \n",
    "         label='MSE', linewidth=2)\n",
    "\n",
    "plt.xlabel('Regularization Parameter (λ)')\n",
    "plt.ylabel('Error')\n",
    "plt.title(f\"Bias-Variance Tradeoff in Regularized Estimation for N'= {Nprime}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Find optimal lambda\n",
    "optimal_lambda_idx = np.argmin(results['mse'])\n",
    "optimal_lambda = lambda_values[optimal_lambda_idx]\n",
    "min_mse = results['mse'][optimal_lambda_idx]\n",
    "\n",
    "# Add annotation for optimal point\n",
    "plt.plot(optimal_lambda, min_mse, 'ro')\n",
    "plt.annotate(f'Optimal λ = {optimal_lambda:.2f}',\n",
    "            xy=(optimal_lambda, min_mse),\n",
    "            xytext=(optimal_lambda + 0.2, min_mse + 0.2),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print numerical results for optimal lambda\n",
    "print(f\"\\nResults at optimal λ = {optimal_lambda:.3f} :\")\n",
    "print(f\"Squared Bias: {results['bias'][optimal_lambda_idx]**2:.3f}\")\n",
    "print(f\"Variance: {results['variance'][optimal_lambda_idx]:.3f}\")\n",
    "print(f\"MSE: {results['mse'][optimal_lambda_idx]:.3f}\")\n",
    "print(f\"B^2+V: {results['bias'][optimal_lambda_idx]**2+results['variance'][optimal_lambda_idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf3b4e-a1f4-4c59-8982-f02949003918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
