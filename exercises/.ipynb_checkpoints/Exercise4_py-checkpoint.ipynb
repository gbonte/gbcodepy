{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae964479-6f4f-4e4f-8963-1a49adfdaa03",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "## Strongly and weakly relevant features\n",
    "# G. Bontempi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974fe6b",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Let us consider a regression task with \n",
    "$N=200$ samples, $n=50$ input features (in the matrix $\\tt{X}$) and one target variable (vector $\\tt{Y}$).\n",
    "\n",
    "Knowing that there are 3 strongly relevant variables and 2 weakly relevant variables,\n",
    "the student has to define and implement a strategy to find them.\n",
    "\n",
    "No existing feature selection code has to be used. However, \n",
    "the student may use libraries to implement supervised learning algorithms.\n",
    "\n",
    "The student code should \n",
    "\n",
    "* return the position of the 3 strongly relevant variables and 2 weakly relevant variables,\n",
    "* discuss what strategy could have been used if the number\n",
    "of strongly and weakly variables was not known in advance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac2cae",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "Let us see first how the input-output dataset was generated.\n",
    "The knowledge of the stochastic process\n",
    "generating the data will allow us to define the correct set of strongly and weakly relevant features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80677e2",
   "metadata": {
    "name": "data generation"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.io\n",
    "\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize parameters\n",
    "N = 200\n",
    "n = 50\n",
    "strong = [0, 6, n-1]  # Converting to 0-based indexing\n",
    "weak = [7, 8]  # Converting to 0-based indexing\n",
    "irr = list(set(range(n)) - set(strong) - set(weak))\n",
    "ns = len(strong)\n",
    "nw = len(weak)\n",
    "\n",
    "# Generate random data\n",
    "Xw = np.random.normal(size=(N, nw))\n",
    "X = np.random.normal(size=(N, n))\n",
    "\n",
    "# Create strong features\n",
    "X[:, strong[0]] = np.sum(np.abs(Xw), axis=1) + np.random.normal(0, 0.1, N)\n",
    "X[:, strong[1]] = np.prod(np.abs(Xw), axis=1) + np.random.normal(0, 0.1, N)\n",
    "X[:, strong[2]] = np.log(np.prod(np.abs(Xw), axis=1)) + np.random.normal(0, 0.1, N)\n",
    "\n",
    "# Set weak features\n",
    "X[:, weak] = Xw\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Generate response variable Y\n",
    "Y = np.sum(np.abs(X[:, strong]), axis=1) + np.random.normal(0, 0.1, N)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b137cfb",
   "metadata": {},
   "source": [
    "The relationship between ${\\mathbf X}=\\{{\\mathbf x_1},{\\mathbf x_2},\\dots,{\\mathbf x_{50}}\\}$\n",
    "and ${\\mathbf y}$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:model}\n",
    "{\\mathbf y}=|x_1+x_7+x_{50}|+{\\mathbf w}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca091a1",
   "metadata": {},
   "source": [
    "where  ${\\mathbf x_1}=| x_8+x_9|+{\\mathbf w_1}$, \n",
    "${\\mathbf x_7}=| x_8 x_9|+{\\mathbf w_7}$, \n",
    "${\\mathbf x_{50}}=\\log | x_8 x_9| +{\\mathbf w_{50}}$\n",
    "and ${\\mathbf w},{\\mathbf w_1},{\\mathbf w_7},{\\mathbf w_{50}}$, are all Normal with zero mean and standard deviation $0.1$.\n",
    "\n",
    "## Definition of strongly and weakly relevant features\n",
    "\n",
    "In the course a strongly relevant feature is defined as a feature ${\\mathbf x}_j$ such that\n",
    "$$ I({\\mathbf X}_{-j},{\\mathbf y})< I({\\mathbf X},{\\mathbf y})$$ or equivalently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef68763",
   "metadata": {},
   "source": [
    "$$ H({\\mathbf y}| {\\mathbf X}_{-j})> H({\\mathbf y}|{\\mathbf X})$$\n",
    "By removing a strongly relevant feature from the input set, the conditional variance of ${\\mathbf y}$ increases.\n",
    "\n",
    "From the model definition it follows that \n",
    "$$ p(y| X)=p(y| x_1,x_7,x_{50})$$\n",
    "or equivalently that ${\\mathbf y}$\n",
    "is conditionally independent of all the other variables when the value of $\\{x_1,x_7,x_{50}\\}$ is known.\n",
    "\n",
    "The set of strongly relevant variables (which is also the Markov blanket) is  then $\\{{\\mathbf x_1},{\\mathbf x_7},{\\mathbf x_{50}}\\}$. \n",
    "\n",
    "A weakly relevant feature is a feature  ${\\mathbf x_j}$ that is not strongly relevant and such that \n",
    "$$ I({\\mathbf S}_{-j},{\\mathbf y})< I({\\mathbf S},{\\mathbf y})$$ \n",
    "or equivalently \n",
    "$$ H({\\mathbf y}| {\\mathbf S}_{-j})> H({\\mathbf y}|{\\mathbf S})$$\n",
    "for a certain context $S \\subset X$. If we consider $S= X \\setminus \\{x_1,x_7,x_{50}\\}$ then \n",
    "$$ p(y| S)=p(y| x_8,x_9)$$\n",
    "It follows that ${\\mathbf y}$\n",
    "is conditionally independent of all the other features of $S$ when the value of $\\{x_8,x_9\\}$ is known.\n",
    "\n",
    "The set of weakly relevant variables is  then $\\{{\\mathbf x_8},{\\mathbf x_9}\\}$. \n",
    "\n",
    "In other terms the set of weakly relevant variables $\\{x_8,x_9\\}$  provides information about ${\\mathbf y}$ for some contexts, e.g. the contexts where $\\{x_1,x_7,x_{50}\\}$ are not available.\n",
    "\n",
    "All the other features are irrelevant since they play no role in the dependency between ${\\mathbf X}$ and ${\\mathbf y}$.\n",
    "\n",
    "## Data-driven estimation of conditional entropy\n",
    "\n",
    "In the real setting (i.e. the one the student is confronted with) the conditional probability and the relationships between input features is not accessible.\n",
    "It is not then possible to compute analytically the information or the entropy terms.\n",
    "\n",
    "Nevertheless, it is possible to estimate the conditional probability  $p(y|S)$\n",
    "and consequently the conditional entropy term $H({\\mathbf y}| {\\mathbf S})$ for a subset $S$ of \n",
    "features by making some assumptions:\n",
    "\n",
    "1. we have a learning model able to return an unbiased and low variant estimation\n",
    "of the regression function. In this case the estimated MISE returns  a good approximation of the conditional variance (i.e. the noise variance) \n",
    "2. the conditional probability is Gaussian. In this case there is a direct link between the conditional variance and the conditional entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae88527",
   "metadata": {},
   "source": [
    "In other terms we make the assumption that \n",
    "$$H({\\mathbf y}| {\\mathbf S_1}) < H({\\mathbf y}| {\\mathbf S_2}) $$ if\n",
    "$$\\widehat{\\text{MISE}_1}< \\widehat{\\text{MISE}_2}$$\n",
    "where $\\widehat{\\text{MISE}_i}$ is the estimated (e.g. by leave-one-out) generalization\n",
    "error of a learner trained with the input set $S_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b0e3e",
   "metadata": {},
   "source": [
    "## Data-driven identification of strongly relevant features \n",
    "\n",
    "Here we identify in a data-driven manner the set of strongly relevant features by\n",
    "choosing as learner a Random Forest and by using a holdout strategy to estimate\n",
    "the generalization error. \n",
    "\n",
    "In practice, we \n",
    "\n",
    "1. remove a single input feature at the time, \n",
    "2. split the dataset in training and validation set and learn a Random Forest with the training set\n",
    "3. compute the Random Forest generalization error for the validation set\n",
    "4. rank the features to select the ones that induced a largest increase of the generalization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d439c3",
   "metadata": {
    "message": false,
    "name": "identification of strongly relevant features",
    "warning": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0741982883175096\n",
      "Strongly relevant identified= [49  0  6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "# Split indices for training and validation\n",
    "Itr = np.random.choice(N, size=round(N/2), replace=False)\n",
    "Ival = np.array([i for i in range(N) if i not in Itr])\n",
    "\n",
    "# Function to mimic R's pred function with RandomForest\n",
    "def pred_rf(X_train, y_train, X_test, class_output=False):\n",
    "    \n",
    "    rf = RandomForestRegressor(n_estimators=500)\n",
    "    rf.fit(X_train, y_train)\n",
    "    return rf.predict(X_test)\n",
    "    \n",
    "\n",
    "# Initial prediction and MISE computation\n",
    "Yhat = pred_rf( X[Itr], Y[Itr], X[Ival])\n",
    "Ehat = (Y[Ival] - Yhat) ** 2\n",
    "MISEhat = ('Testing error=',np.mean(Ehat)]\n",
    "\n",
    "print(np.mean(MISEhat ** 2))\n",
    "\n",
    "# Initialize arrays for feature importance computation\n",
    "MISEhatj = np.zeros(n)\n",
    "Ehatj = np.full((len(Ival), n), np.nan)\n",
    "\n",
    "# Compute MISE for each feature removed\n",
    "for j in range(n):\n",
    "    # Create views of X without column j\n",
    "    X_tr_reduced = np.delete(X[Itr], j, axis=1)\n",
    "    X_val_reduced = np.delete(X[Ival], j, axis=1)\n",
    "    \n",
    "    Yhatj = pred_rf( X_tr_reduced, Y[Itr], X_val_reduced)\n",
    "    Ehatj[:, j] = (Y[Ival] - Yhatj) ** 2\n",
    "    MISEhatj[j] = np.mean(Ehatj[:, j])\n",
    "\n",
    "# Find strongly relevant features\n",
    "diff_MISE = MISEhatj - MISEhat\n",
    "stronghat = np.argsort(diff_MISE)[::-1][:ns]\n",
    "\n",
    "print(f\"Strongly relevant identified= {stronghat+1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32973f70",
   "metadata": {},
   "source": [
    "According to the procedure above, by knowing that there are _ns_ strongly relevant variables, the set of strongly relevant variables is in the columns _stronghat_ of the input matrix $X$.\n",
    "\n",
    "## Data-driven identification of weakly relevant features \n",
    "\n",
    "The identification of weakly relevant variables would need a search in the space of all \n",
    "possible contexts. Here we limit to consider the context $S= X \\setminus \\{x_1,x_7,x_{50}\\}$ obtained by removing the strongly \n",
    "relevant features from the input set. The hold-out procedure is similar to the one in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d5b7c6",
   "metadata": {
    "message": false,
    "name": "identification of weakly relevant features",
    "warning": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55015245 0.0712505 ]\n",
      "Weakly relevant identified= [8 9]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make prediction excluding strong features\n",
    "Yhat = pred_rf(X[Itr][:, [i for i in range(X.shape[1]) if i not in strong]], Y[Itr], \n",
    "            X[Ival][:, [i for i in range(X.shape[1]) if i not in strong]])\n",
    "\n",
    "# Calculate mean squared error\n",
    "wMISEhat = np.mean((Y[Ival] - Yhat) ** 2)\n",
    "\n",
    "\n",
    "# Initialize array for storing MSE values\n",
    "wMISEhatj = np.full(n, -100, dtype=float)\n",
    "\n",
    "# Calculate MSE for each feature (excluding strong features)\n",
    "for j in [x for x in range(n) if x not in strong]:\n",
    "    # Create mask for columns to exclude (both strong features and current feature j)\n",
    "    cols_to_exclude = strong + [j]\n",
    "    mask = [i for i in range(X.shape[1]) if i not in cols_to_exclude]\n",
    "    \n",
    "    # Make prediction without current feature\n",
    "    Yhatj = pred_rf( X[Itr][:, mask], Y[Itr], X[Ival][:, mask])\n",
    "    wMISEhatj[j] = np.mean((Y[Ival] - Yhatj) ** 2)\n",
    "\n",
    "# Find weakly relevant features\n",
    "differences = wMISEhatj - wMISEhat\n",
    "weakhat = np.argsort(differences)[::-1][:nw]  # Sort in descending order and get top nw indices\n",
    "print(np.sort(differences)[::-1][:nw])  # Print the top nw differences\n",
    "print(f\"Weakly relevant identified= {weakhat+1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f495136f",
   "metadata": {},
   "source": [
    "According to the procedure above we see that there are _nw_ features that,\n",
    "once removed, increase the generalization error of the context $S= X \\setminus \\{x_1,x_7,x_{50}\\}$. We may deduce then that the set of weakly relevant variables is in the columns _weakhat_ of the input matrix $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096254ca",
   "metadata": {},
   "source": [
    "## What to do in the general case\n",
    "\n",
    "The solution in this exercise has been facilitated by the knowledge of\n",
    "the number of strongly and weakly relevant features. Unfortunately, this information is hardly\n",
    "available in real settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9017c4",
   "metadata": {},
   "source": [
    "The main issue related to identification of relevant features is that\n",
    "we cannot compute the analytical exact value of the conditional entropy (or conditional\n",
    "information terms) because of the stochastic finite-data setting.\n",
    "In practice we have only rough estimates of those terms.\n",
    "Nevertheless, most of the time we are not interested in the actual values\n",
    "of those terms but in their relative values: for instance we may be interested \n",
    "to know if\n",
    "$$H({\\mathbf y}| {\\mathbf S_1}) < H({\\mathbf y}| {\\mathbf S_2}) $$ \n",
    "of if their difference is smaller than zero.\n",
    "\n",
    "Since those values are only estimated the fact that \n",
    "$$\\hat{H}({\\mathbf y}| {\\mathbf S_1}) < \\hat{H}({\\mathbf y}| {\\mathbf S_2}) $$\n",
    "does not necessarily provide enough evidence to draw a conclusion. Given the stochastic setting\n",
    "a solution could be the adoption of statistical tests. For instance \n",
    "if $H$ is approximated by $\\widehat{\\text{MISE}}$ we could use a statistical test\n",
    "to check whether the mean $\\widehat{\\text{MISE}_1}$ is significantly smaller than $\\widehat{\\text{MISE}_2}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba9491",
   "metadata": {},
   "source": [
    "Let us see how this could be done in practice.\n",
    "\n",
    "### Data-driven identification of the number of strongly relevant features \n",
    "\n",
    "In this case we do not know exactly where to stop in the decreasing ranking of the vector $\\tt{MISEhatj-MISEhat}$.\n",
    "\n",
    "In what follows we use a t-test comparing the vector of test errors \n",
    "(stored in the R variable $\\tt{Ehatj}$) of \n",
    "each feature set $X_{-j}$ to the the one of $X$ (stored in the R variable $\\tt{Ehat}$). This checks if\n",
    "the mean $\\widehat{\\text{MISE}_{-j}}$ is significantly\n",
    "larger (pvalue smaller than $0.01$) than $\\widehat{\\text{MISE}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bfe2ff6",
   "metadata": {
    "name": "selection number strongly relevant features"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stronghat_test= [ 1  7 50]\n",
      "Index: 7, P-value: 0.0004241549364749579\n",
      "Index: 50, P-value: 0.0008818805852963087\n",
      "Index: 1, P-value: 0.003359186787547234\n",
      "Index: 35, P-value: 0.016935582494711807\n",
      "Index: 38, P-value: 0.02415069349899065\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "pv = np.zeros(n)\n",
    "for j in range(n):\n",
    "    # Perform paired t-test and extract p-value\n",
    "    t_stat, pv[j] = stats.ttest_rel(Ehatj[:,j], Ehat, alternative='greater')\n",
    "\n",
    "# Find indices where p-value is less than 0.01\n",
    "stronghat_test = np.where(pv < 0.01)[0]\n",
    "\n",
    "print('stronghat_test=',stronghat_test+1)\n",
    "# Sort p-values and get their indices\n",
    "sorted_indices = np.argsort(pv)[:5]\n",
    "sorted_pvalues = pv[sorted_indices]\n",
    "# Print sorted p-values and their indices\n",
    "for idx, p_val in zip(sorted_indices, sorted_pvalues):\n",
    "    print(f\"Index: {idx+1}, P-value: {p_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e564c2",
   "metadata": {},
   "source": [
    "It follows that (for the given pvalue threshold) the set of strongly relevant features is _stronghat.test_. Of course this number could be different for\n",
    "different pvalue thresholds.\n",
    "\n",
    "### Data-driven identification of the number of weakly relevant features \n",
    "\n",
    "The procedure above can be used as well for detecting weakly relevant features\n",
    "for a given context.\n",
    "Nevertheless, since the number of weakly features is not given in advance, the problem of finding the set of weakly relevant features would remain much harder. \n",
    "In fact, we are not supposed to stop the search until we have not considered all the possible contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc647a7e-4a9d-4828-8a8b-58e685fd3c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "message,warning,name,tags,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
